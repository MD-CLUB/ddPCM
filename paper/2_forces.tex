\section{Computation of the ddPCM-Forces}\label{sec:forces}

\subsection{Theory}
%In the case of a classical solute, the form of the charge density $\rho$ allows to compute 
The solvation energy can be written as a sum of subdomain contributions, which perfectly fits the domain-decomposition paradigm. 

First, for a classical solute's charge distribution of the form of $\rho=\sum_j q_j \delta_{x_j}$, we can develop
\[
	E_s 
	= \frac{1}{2} \,  \int_{\Omega} \rho(x) W(x) \, dx
	= \frac{1}{2} \, \sum_j q_j  W(x_j)
	= 2 \pi \, \sum_j q_j [X_j]_0^0    \, Y_0^0
	= {\sqrt{\pi}}\, \sum_j q_j [X_j]_0^0
\]
This concept easily generalizes to point multipolar charge distributions. 
The evaluation of the energy for charge distributions is a bit more involved as it requires a three-dimensional integration and we refer to \cite{Lipparini_JCP_ddCOSMO-QM} for more details. In all cases however, the energy can be written as
\[
E_s = \tfrac{1}{2}
% \,f(\varepsilon)
 \, \sum_j \sum_{\ell,m} [\Psi_j]_\ell^m [X_j]_\ell^m
  =: \tfrac{1}{2} 
  %\,f(\varepsilon) 
  \,\langle \Psi, X \rangle
\]
where the angular brackets indicate the double scalar product over $j$ and $\ell,m$.
For example, for the classical charge as illustrated above, we have 
\[
	[\Psi_j]_\ell^m = {\sqrt{\pi}}\, q_j \, \delta_{\ell,0} \delta_{m,0}.
\]

% THE FOLLOWING IS NOT ENTIRELY CORRECT SINCE THE OVERLAP IN THE INTEGRALS SHOULD BE SUBSTRACTED
%Indeed, the spherical harmonics addition theorem implies that
%\[
%W_j(x) = (\tilde{\cS}_j \, \sigma_j)(x) = \sum_{\ell,m} [X_j]_\ell^m (\tilde{\cS}_j \, Y_\ell^m)(x) = \sum_{\ell,m} [X_j]_\ell^m \, \frac{4\pi}{2\ell+1} \,\frac{r^\ell}{r_j^{\ell+1}} \, Y_\ell^m(y)
%\]
%where $x = x_j + r\, y$, so that the solvation energy can be determined as
%\[
%E_s = \tfrac{1}{2} 
%%\,f(\varepsilon) 
%\, \sum_{j=1}^M \int_{\Omega_j} \rho(x) W_j(x) \, dx = \tfrac{1}{2}
%%\, f(\varepsilon) 
%\, \sum_{j=1}^M \sum_{\ell,m} [X_j]_\ell^m \, \frac{4\pi}{2\ell+1} \,\frac{1}{r_j^{\ell+1}} 
%\int_{\Omega_j} \rho(x) \, r^\ell \, Y_\ell^m(y) \, dx
%\]
%If we define
%\[
%[\Psi_j]_\ell^m = \frac{4\pi}{2\ell+1}\, \frac{1}{r_j^{\ell+1}}\int_{\Omega_j} \rho(x) \, r^\ell \, Y_\ell^m(y) \, dx
%\]
%we can compactly write the energy as
%\[
%E_s = \tfrac{1}{2}
%% \,f(\varepsilon)
% \, \sum_j \sum_{\ell,m} [\Psi_j]_\ell^m [X_j]_\ell^m
%  =: \tfrac{1}{2} 
%  %\,f(\varepsilon) 
%  \,\langle \Psi, X \rangle
%\]
%where the angular brackets indicate the double scalar product over $j$ and $\ell,m$.

The force acting on the $i$-th atom is then given by
\[
\mathcal{F}_i = -\nablai E_s = - \tfrac{1}{2} 
%\,f(\varepsilon) 
\,  \langle \Psi, \nablai X \rangle 
% = - \langle \Psi, \nabla_j\sigma \rangle
\]
where the gradient $\nablai$ is understood with respect to $x_i$ and $X$ denotes the solution to the ddPCM-equations~\eqref{eq:6}. 
Here we used the fact that the entries of the vector $\Psi$ are independent of the atomic positions. On the other hand, since both the matrices $A_\varepsilon$, $A_\infty$ and the right-hand side $F$ depend on the nuclear positions $x_1 , \ldots, x_M$, so does the unknown $X$ of~\eqref{eq:6}, which can compactly be written as $A_\varepsilon \, L \, X = A_\infty \, F$.
The idea adjoint differentiation is to consider the adjoint problem $(A_\varepsilon \, L)^* s = \Psi$ and compute the quantity $\langle \Psi, \nablai X \rangle = \langle s ,  A_\varepsilon \, L \, \nablai X\rangle$ by using the definition of the adjoint matrix.

%If combine equations \eqref{eq:6}, the fully discretized problem becomes $A_\varepsilon \, L \, X = A_\infty \, F$, and 
Applying the Leibnitz differentiation rule to $A_\varepsilon \, L \, X = A_\infty \, F$ allows to move the derivative from $X$ onto the other terms, namely
\[
A_\varepsilon \, L \, \nablai X = \nablai A_\infty \, F +  A_\infty \, \nablai F - \nablai A_\varepsilon \, L \, X -  A_\varepsilon \, \nablai L \, X=: h_i
\]
Thus, once the solution $s$ of the adjoint problem and vector $h_i$ have been determined, the forces can be computed as
\[
\mathcal{F}_i =  - \tfrac{1}{2} 
%\,f(\varepsilon)
 \,  \langle s, h_i \rangle
\]
The derivatives $\nablai L $ of the ddCOSMO discretization were discussed in \cite{Lipparini_JCTC_ddCOSMO} and we now focus on the new parts due to the ddPCM-method. We remark that the quantity $\nablai F$ is \emph{a priori} nonzero since $F_j$ does depend upon the nuclear positions through the characteristic function $U_j$, recall \eqref{eq:25}.

Let $\{ s_n\}$ be the $N_\text{grid}$ Lebedev integration points with associated weights $\{ w_n \}$ and define the following quantities
\[
t_n^{jk} = \frac{|x_j + r_j s_n -x_k|}{r_k} \quad , \quad s_n^{jk} = \frac{x_j + r_j s_n -x_k}{|x_j + r_j s_n -x_k|}\quad , \quad U_j^n = U_j(x_j + r_j s_n)
\]
The entries of $F_j$ are evaluated through numerical quadrature as
\[
[F_j]_\ell^m = - \sum_{n = 1} ^{N_\text{grid}} w_n \, U_j^n \, \Phi(x_j + r_js_n) \, Y_\ell^m(s_n)
\]
so that we immediately obtain
\[
[\nablai F_j]_\ell^m = - \sum_{n = 1} ^{N_\text{grid}} w_n \, \nablai U_j^n \, \Phi(x_j + r_js_n) \, Y_\ell^m(s_n)
\]
In the remainder of this section we discuss the derivatives of the ddPCM matrix $A_\varepsilon$.

The blocks $A_{jk}^\varepsilon$ of the ddPCM matrix $A_\varepsilon$, see \eqref{eq:ajj} and \eqref{eq:ajk}, have the form
\begin{alignat*}{3}
{[A_{jj}^\varepsilon]}_{\ell \ell'}^{mm'}& = 2\pi \, \frac{\varepsilon + 1}{\varepsilon - 1}\, \delta_{\ell \ell'} \delta_{m m'}&& + \frac{2\pi}{2 \ell' + 1} \,\sum_{n= 1}^{N_\text{grid}} w_n \, U_j^n  \,Y_\ell^m(s_n) \,  Y_{\ell'}^{m'}(s_n) \\
{[A_{jk}^\varepsilon]}_{\ell \ell'}^{mm'}& =&& -  \frac{4 \pi \ell'}{2 \ell'+1} \, \sum_{n= 1}^{N_\text{grid}} w_n\, U_j^n  \, Y_\ell^m(s_n) \, \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk})
\end{alignat*}
and, since the derivatives are independent of $\varepsilon$, we drop the $\varepsilon$-dependency for ease of notation.

The case of the diagonal blocks yields
\[
{[\nablai A_{jj}]}_{\ell \ell'}^{mm'} = \frac{2\pi}{2 \ell' + 1} \,\sum_{n} w_n \, \nablai U_j^n  \,Y_\ell^m(s_n) \,  Y_{\ell'}^{m'}(s_n)
\]
so that it only requires the derivatives of the characteristic function. The function $U_j$ is, in practice, a smoothed version of the (discontinuous!)~characteristic function, and is defined as 
\[
U_j(x_j + r_j y) =
\begin{cases}
1 - f_j(y) 	&\quad f_j(y) \le 1\\
0		&\quad \text{otherwise}
\end{cases}
\qquad , \qquad 
f_j(y) = \sum_{k \in N_j} \chi \bigg(\frac{|x_j + r_j y - x_k|}{r_k}\bigg)
\]
where $y$ varies on $\mathbb{S}$ and $\chi$ is a regularized characteristic function of $[0,1]$. We conclude that $\nablai U_j$ and, consequently, $\nablai A_{jj}$ are \emph{a priori} nonzero only when $i \in N_j$ or $i = j$.

The case of the off-diagonal blocks ${[ A_{jk}]}_{\ell \ell'}^{mm'}$ with $j \not=k$ is more involved since it includes the gradient of the product of three functions, namely
\begin{equation}\label{eq:7}
{[\nablai A_{jk}]}_{\ell \ell'}^{mm'} = -  \frac{4 \pi \ell'}{2 \ell'+1} \, \sum_{n} w_n\, Y_\ell^m(s_n) \,\nablai \Big[ U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk}) \Big]
\end{equation}
%\cdots\Big[ \nabla U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk}) + U_j^n  \, \nabla \Big( \big( t_n^{jk}\big)^{-(\ell'+1)} \Big)\, Y_{\ell'}^{m'} (s_n^{jk}) + U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, \nabla Y_{\ell'}^{m'} (s_n^{jk}) \Big]
%\end{multline*}
However, since $t_n^{jk}$ and $s_n^{jk}$ depend only upon $x_j$ and $x_k$, if we assume $i \not=j$ and $i\not=k$, we obtain
\begin{equation}\label{eq:8}
{[\nablai A_{jk}]}_{\ell \ell'}^{mm'} = -  \frac{4 \pi \ell'}{2 \ell'+1} \, \sum_{n} w_n\, Y_\ell^m(s_n) \,\nablai U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk})
\end{equation}
%Such relation holds, in particular, for $i \in N_j$ and $i \not= k$.
Thus, since $U_j^n$ depends only upon $x_i$ if $i \in N_j$ or $i=j$, we conclude that $\nablai A_{jk}$ vanishes whenever $i \not= j$ and $i \not=k$ and $i \not\in N_j$. In order to discuss the opposite case, i.e., $i = j$ or $i =k$ or $i \in N_j$, notice that the events $(i = j)$ and $(i = k)$ are mutually exclusive, as are $(i = j)$ and $(i \in N_j)$. We obtain the three subcases $i = j$, and $i = k$, and $i \in N_j \, , \, i \not= k$, which will be addressed individually.

Standard differentiation implies that
\begin{multline}\label{eq:9}
\nablai \Big[ U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk}) \Big] = \nablai U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk}) \\
- U_j^n  \, (\ell' + 1)  \big( t_n^{jk}\big)^{-(\ell'+2)} \, \nablai t_n^{jk} \, Y_{\ell'}^{m'} (s_n^{jk}) + U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, \big( \Di\, s_n^{ji} \big)^T\, \nablai Y_{\ell'}^{m'} (s_n^{jk})
\end{multline}
where $\Di$ emphasizes that the gradient of the vector quantity $s_j^{jk}$ is indeed its Jacobian matrix and where the extra subscripts refer to the variables with respect to which differentiation is taken. We proceed to evaluate $\nablai t_n^{jk}$ and $ \Di \, s_n^{jk}$. When $i = j$, differentiation implies
\[
\nablaj t_n^{jk} = \frac{s_n^{jk}}{r_k} \qquad , \qquad \Dj \, s_n^{jk} = \frac{I - s_n^{jk} \otimes s_n^{jk} }{|x_j + r_j s_n - x_k|^3}
\]
where $I$ is the identity matrix and $\otimes$ indicates the outer product. We remark that the Jacobian matrix $\Dj \, s_n^{jk}$ is symmetric, so that the transpose in \eqref{eq:9} is redundant. 
Due to the particular relation between $x_j$ and $x_k$, we obtain $\nabla_{\! j} \, t_n^{jk} = - \nabla_{\! k} \, t_n^{jk}$ and $D_j \, s_n^{jk} = - D_k \, s_n^{jk}$.
We can therefore analogously derive the case $i = k$ and those relationships imply
\begin{multline*}
\nabla_{\! j} \Big[ U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk}) \Big] + \nabla_{\! k} \Big[ U_j^n  \,  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk}) \Big] = \\
  \Big[ \nabla_{\! j} \, U_j^n + \nabla_{\! k} \, U_j^n  \Big]  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk})
\end{multline*}
which provide a convenient way of evaluating $[\nabla_{\! k} \, A_{jk}]_{\ell \ell'}^{m m'}$ from $[\nabla_{\! j} \, A_{jk}]_{\ell \ell'}^{m m'}$. In fact, we obtain the quasi-skew-symmetric relation
\[
[ \nabla_{\! j} \, A_{jk}]_{\ell \ell'}^{m m'} + [\nabla_{\! k} \, A_{jk}]_{\ell \ell'}^{m m'} = -  \frac{4 \pi \ell'}{2 \ell'+1} \, \sum_{n} w_n\, Y_\ell^m(s_n) \Big[ \nabla_{\! j} \, U_j^n + \nabla_{\! k} \, U_j^n  \Big]  \big( t_n^{jk}\big)^{-(\ell'+1)} \, Y_{\ell'}^{m'} (s_n^{jk})
\]
for $\nablai A_{jk}$. Finally, the case $i \in N_j \, , \, i \not= k$ reduces to \eqref{eq:8}.

\bigskip

\noindent
{\bf A quick overview of sparsity of $\nablai A_{jk}$}: There is nothing new, but should clarify a bit the computational complexity of computing the forces.
\begin{itemize}
\item {\bf Diagonal terms: $j=k$:}
	\begin{itemize}
		\item If $i\in N_j=N_k$ or $i=j=k$, then $[ \nablai A_{jj}]_{\ell \ell'}^{m m'} \neq 0$. 
		\item If $i\not \in N_j=N_k$ and  $i\neq j=k$, then $[ \nablai A_{jk}]_{\ell \ell'}^{m m'} = 0$.
	\end{itemize}
\item {\bf Off-diagonal terms: $j\neq k$:}
	\begin{itemize}
		\item If $i=k$, then $[ \nablai A_{jk}]_{\ell \ell'}^{m m'} \neq 0$.
		\item If $i=j$, then $[ \nablai A_{jk}]_{\ell \ell'}^{m m'} \neq 0$.
		\item If $i\neq j$ and $i\neq k$
		\begin{itemize}
			\item If $i\in N_j$ and $i\neq k$, then $[ \nablai A_{jk}]_{\ell \ell'}^{m m'} \neq 0$.
			\item If $i\not\in N_j$ and $i\neq k$, then $[ \nablai A_{jk}]_{\ell \ell'}^{m m'} =0 0$.
		\end{itemize}
	\end{itemize}
\end{itemize}
From a different perspective, we want now to analyse the complexity of the matrix-vector product $\nablai A f$ for some vector $f$.
%

We see that 
\begin{align*}
	[(\nablai A f)_j]_\ell^m 
	&= \sum_k \sum_{\ell',m'} [\nablai A_{jk}]_{\ell \ell'}^{m m'} [f_k]_{\ell'}^{m'} \\
	&= \sum_{\ell',m'} [\nablai A_{jj}]_{\ell \ell'}^{m m'} [f_j]_{\ell'}^{m'} 
	+ \sum_{k\neq j} \sum_{\ell',m'} [\nablai A_{jk}]_{\ell \ell'}^{m m'} [f_k]_{\ell'}^{m'} 
\end{align*}
so that we split $\nablai A$ into the block-diagonal part $\nablai A_d$ and the off-diagonal part $\nablai A_o$.
\begin{align*}
	[(\nablai A_d f)_j]_\ell^m 
	&= \sum_{\ell',m'} [\nablai A_{jj}]_{\ell \ell'}^{m m'} [f_j]_{\ell'}^{m'} \\
	[(\nablai A_o f)_j]_\ell^m 
	&= \sum_{k\neq j} \sum_{\ell',m'} [\nablai A_{jk}]_{\ell \ell'}^{m m'} [f_k]_{\ell'}^{m'} 
\end{align*}
For the diagonal part, we have that
\[
	[(\nablai A_d f)_j]_\ell^m = 0
\]
if $i\not\in N_j$ and $i\neq j$. For a fixed $i$, the number of non-zero contributions (indexed by $j$) from the diagonal is independent of $M$. $\mathcal O(1)$

For the off-diagonal part, we distinguish three cases.

Case 1 ($i=j$ and necessarily $i\neq N_j$):
\[
	[(\nablai A_o f)_i]_\ell^m 
	= 	\sum_{k\neq i} \sum_{\ell',m'} [\nablai A_{ik}]_{\ell \ell'}^{m m'} [f_k]_{\ell'}^{m'} 
\]
Here, all terms $k\neq i$ must be considered. This operation is $\mathcal O(M)$ but this case happens for one $j$ only.

Case 2 ($i\neq j$ and $i\in N_j$):
\[
	[(\nablai A_o f)_j]_\ell^m 
	= \sum_{\ell',m'} [\nablai A_{ji}]_{\ell \ell'}^{m m'} [f_i]_{\ell'}^{m'} 
	+ \sum_{k\neq j, k\neq i} \sum_{\ell',m'} [\nablai A_{jk}]_{\ell \ell'}^{m m'} [f_k]_{\ell'}^{m'} 
\]
This operation is $\mathcal O(M)$, but this case happens for $\mathcal O(1)$ values of $j$ only.

Case 3 ($i\neq j$ and $i\not \in N_j$):
\[
	[(\nablai A_o f)_j]_\ell^m 
	= \sum_{\ell',m'} [\nablai A_{ji}]_{\ell \ell'}^{m m'} [f_i]_{\ell'}^{m'} 
\]
This operation is $\mathcal O(1)$ and this needs to be done for $\mathcal O(M)$ values of $j$.

Therefore, we see that computing all coefficients of the matrix-vector product $[(\nablai A f)_j]_\ell^m $ for one fixed $i$ requires $\mathcal O(M)$ of operations. 
Now, this needs to be done for each $i=1,\ldots,M$ such that the overall complexity to compute all coefficients $[(\nablai A f)_j]_\ell^m $ for all $i$ and $j$ is $\mathcal O(M^2)$.

\subsection{Implementation}
We solve the adjoint problem $(A_\varepsilon \, L )^* s = \Psi$ through the two steps ${L}^* \, y = \Psi$ and ${A_\varepsilon}^* \, s = y$, and apply Leibnitz differentiation rule to the PCM problem \eqref{eq:6}, so that
\[
\nablai A_\varepsilon \, G + A_\varepsilon \, \nablai G = \nablai A_\infty \, F + A_\infty \, \nablai F  \qquad , \qquad \nablai L \, X + L \, \nablai X = \nablai G
\]
Since $\nablai A_\infty=\nablai A_\varepsilon =: \nablai A$, we obtain
\begin{align*}
\langle \Psi , \nablai X\rangle &  = \langle s ,  A_\varepsilon \, L \, \nablai X\rangle \\
&  = \langle s ,  A_\varepsilon \,( \nablai G - \nablai L \,  X ) \rangle \\ 
& = \langle s ,  A_\varepsilon \, \nablai G \rangle - \langle s , A_\varepsilon \, \nablai L \,  X \rangle \\
& = \langle s , \nablai  A ( F - G ) + A_\infty \, \nablai F \rangle - \langle s , A_\varepsilon \, \nablai L \,  X \rangle \\
& = \langle s , \nablai  A ( F - G ) \rangle + \langle {A_\infty}^* \, s , \nablai F \rangle - \langle {A_\varepsilon}^* \, s , \nablai L \,  X \rangle \\
& = \langle s , \nablai  A ( F - G ) \rangle + \langle {A_\infty}^* \, s , \nablai F \rangle - \langle y , \nablai L \,  X \rangle\\
& = \langle s , \nablai  A ( F - G ) \rangle + \langle ({A_\infty}^*-{A_\varepsilon}^*) \, s , \nablai F \rangle + \langle y, \nablai F-\nablai L \,  X \rangle 
\end{align*}
Notice that, when $\varepsilon = \infty$, then $G = F$ and ${A_\infty}^* \, s =y$, so that we recover
\[
\langle \Psi , \nablai X\rangle =  \langle y , \nablai F \rangle - \langle y , \nablai L \,  X \rangle
\]
which is the expression of the force for COSMO. Because of the sparsity of $L$, the computation of the action $\nablai L \, X$ is of complexity $O(N_\text{grid})$, with a prefactor that depends, al least, upon $N_i$ and $L_\text{max}$. Since the contraction product $\langle \cdot \, , \cdot \rangle$ also has complexity $O(M)$, with a prefactor that depends on $L_\text{max}$, we conclude that the cost is $O(N_\text{gird} \times M)$. In the case of PCM, the bottleneck is the action $ \nablai  A ( F - G)$...

%1. Use that $\nablai A_\infty=\nablai A_\varepsilon$ and that $L \, X=G$:
%\begin{alignat*}{1}
%h_i &= \nablai A_\infty \, F +  A_\infty \, \nablai F - \nablai A_\varepsilon \, L \, X -  A_\varepsilon \, \nablai L \, X\\
%&= \nablai A \, (F-  G) +  A_\infty \, \nablai F  -  A_\varepsilon \, \nablai L \, X
%\end{alignat*}
%
%2. Explain how to do the following operations efficiently:
%\begin{alignat*}{3}
%\langle s, \nablai A \, (F-  G) \rangle &= \langle (\nablai A)^* \, s,  F-  G \rangle ? \\
%\langle s, A_\infty \, \nablai F \rangle &= \langle A_\infty \,^* \, s,  \nablai F \rangle \\
%\langle s, A_\varepsilon \, \nablai L \, X \rangle &= \langle (A_\varepsilon\, \nablai L)^* \, s,  X \rangle ? \\
%\end{alignat*}
%I don't see how the first and third term are efficiently done in practice?
%
%3. Use the skew-symmetric relationship from above.
